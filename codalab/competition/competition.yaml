---
title: Evaluating grammatical error corrections
description: This "competition" contains different evaluation metrics commonly used for GEC and allows users to score their systems with these metrics on a common dataset.
has_registration: True
html:
    overview: overview.html
    evaluation: evaluation.html
    terms: terms.html
    data: data.html
phases:
    1:
        phasenumber: 1
        label: Evaluation
        start_date: 2016-11-02
        reference_data: reference.zip
        scoring_program: program.zip
        description: Evaluate corrections of the CoNLL-2014 shared task test set with GLEU, GLEU interpolated with LT (lambda optimized to Spearman's rho), and M2. GLEU and M2 are calculated using all available references. View the scoring output log to see the scores from other metrics and reference sets.


leaderboard:
    leaderboards:
        Results: &Results
            label: Results
            rank: 1
    columns:
        GLEU_ALL:
            leaderboard: *Results
            label: GLEU
            rank: 1
        LT_GLEU_ALL_rho:
            leaderboard: *Results
            label: GLEU_intpl
            rank: 2
        M2_ALL:
            leaderboard: *Results
            label: M2
            rank: 3
